from pyspark import SparkConf
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType,StructField,StringType,DoubleType,IntegerType

spark_conf=SparkConf()
spark_conf.set("spark.app.name","assignment")
spark_conf.set("spark.master","local[*]")
spark=SparkSession.builder.config(conf=spark_conf).getOrCreate()

schemaname=StructType([StructField("name",StringType()),StructField("col1",IntegerType()),StructField("col2",IntegerType()),StructField("col3",IntegerType()),StructField("col4",DoubleType())])

df1=spark.read.format("csv").schema(schemaname).option("path","C:\\Users\\SREEJIT\\OneDrive\\Desktop\\spark practice\\windowdata-201021-002706.csv").load()


df1.write.format("parquet").partitionBy("name","col1").mode("overwrite").option("path","C:\\Users\\SREEJIT\\OneDrive\\Desktop\\pyspark_impl").save()












































































































































































































































































































































































































































































































































































































  

